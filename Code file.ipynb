{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a7b455f-d564-4471-a124-f5eaf47bbace",
   "metadata": {},
   "source": [
    "### Extraction and verification of information from semi-categorized data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db0e2160-57af-47ac-9be6-5f52bb6296b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.5381\n",
      "Epoch 2/3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 1.0000 - loss: 1.7355e-04\n",
      "Epoch 3/3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Deleted temporary file: C:/Users/DC/Downloads/EAadhaar_unlocked.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pikepdf\n",
    "import PyPDF2\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, scrolledtext\n",
    "from pdf2image import convert_from_path\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# ‚úÖ Try importing TensorFlow (optional)\n",
    "USE_CNN = True\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "except ImportError:\n",
    "    USE_CNN = False\n",
    "    print(\"‚ö† TensorFlow not installed! CNN training will be skipped.\")\n",
    "\n",
    "# ‚úÖ Function to unlock a password-protected PDF\n",
    "def unlock_pdf(input_pdf_path, password):\n",
    "    output_pdf_path = input_pdf_path.replace('.pdf', '_unlocked.pdf')\n",
    "    try:\n",
    "        with pikepdf.Pdf.open(input_pdf_path, password=password) as pdf:\n",
    "            pdf.save(output_pdf_path)\n",
    "        return output_pdf_path\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Error unlocking PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "# ‚úÖ Function to extract text from Aadhaar PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Error extracting text: {e}\")\n",
    "    return text\n",
    "\n",
    "# ‚úÖ Function to remove Gujarati text (only keep English characters)\n",
    "def filter_text_english_only(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', '', text)  # Removes non-ASCII characters\n",
    "\n",
    "# ‚úÖ Function to convert PDF to images\n",
    "def pdf_to_images(pdf_path, output_folder=\"aadhaar_images\"):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    images = convert_from_path(pdf_path)\n",
    "    image_paths = []\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        img_path = os.path.join(output_folder, f\"aadhaar_page_{i+1}.png\")\n",
    "        img.save(img_path, \"PNG\")\n",
    "        image_paths.append(img_path)\n",
    "\n",
    "    return image_paths\n",
    "\n",
    "# ‚úÖ Function to preprocess images for CNN\n",
    "def preprocess_images(image_paths):\n",
    "    processed_images = []\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (128, 128))  # Resize for CNN\n",
    "        img = img / 255.0  # Normalize pixel values\n",
    "        processed_images.append(img)\n",
    "\n",
    "    return np.array(processed_images).reshape(-1, 128, 128, 1)  # Reshape for CNN\n",
    "\n",
    "# ‚úÖ Function to delete temporary unlocked Aadhaar PDF\n",
    "def delete_temp_pdf(pdf_path):\n",
    "    try:\n",
    "        if os.path.exists(pdf_path):\n",
    "            os.remove(pdf_path)\n",
    "            print(f\"Deleted temporary file: {pdf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting temporary file: {e}\")\n",
    "\n",
    "# ‚úÖ Function to build a CNN model (Only if TensorFlow is installed)\n",
    "def build_cnn_model():\n",
    "    if not USE_CNN:\n",
    "        return None\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(2, activation='softmax')  # 2 classes (Valid/Invalid Aadhaar)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "# ‚úÖ GUI Setup\n",
    "root = tk.Tk()\n",
    "root.title(\"Aadhaar PDF Processor\")\n",
    "root.geometry(\"600x500\")\n",
    "\n",
    "tk.Label(root, text=\"Enter PDF Password (if required):\").pack(pady=5)\n",
    "password_entry = tk.Entry(root, show='*', width=30)\n",
    "password_entry.pack(pady=5)\n",
    "\n",
    "text_area = scrolledtext.ScrolledText(root, wrap=tk.WORD, width=70, height=10)\n",
    "text_area.pack(pady=10)\n",
    "\n",
    "def upload_pdf():\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"PDF files\", \"*.pdf\")])\n",
    "    if not file_path:\n",
    "        messagebox.showwarning(\"Warning\", \"No file selected!\")\n",
    "        return\n",
    "\n",
    "    password = password_entry.get()\n",
    "    unlocked_pdf_path = unlock_pdf(file_path, password) if password else file_path\n",
    "\n",
    "    extracted_text = extract_text_from_pdf(unlocked_pdf_path)\n",
    "    extracted_text = filter_text_english_only(extracted_text)\n",
    "\n",
    "    if extracted_text:\n",
    "        text_area.delete(1.0, tk.END)\n",
    "        text_area.insert(tk.END, \"Extracted Text:\\n\")\n",
    "        text_area.insert(tk.END, extracted_text + \"\\n\")\n",
    "        messagebox.showinfo(\"Success\", \"Text extracted successfully!\")\n",
    "\n",
    "        # ‚úÖ Convert PDF to images\n",
    "        image_paths = pdf_to_images(unlocked_pdf_path)\n",
    "        \n",
    "        # ‚úÖ Preprocess images for CNN\n",
    "        cnn_input_data = preprocess_images(image_paths)\n",
    "\n",
    "        # ‚úÖ If TensorFlow is installed, train CNN\n",
    "        if USE_CNN:\n",
    "            model = build_cnn_model()\n",
    "            if model:\n",
    "                model.fit(cnn_input_data, np.zeros(len(cnn_input_data)), epochs=3)  # Dummy labels for now\n",
    "                messagebox.showinfo(\"Success\", \"CNN Model Trained Successfully!\")\n",
    "        else:\n",
    "            print(\"‚ö† Skipping CNN Training (TensorFlow not installed).\")\n",
    "\n",
    "        # ‚úÖ Delete temporary files\n",
    "        delete_temp_pdf(unlocked_pdf_path)\n",
    "\n",
    "    else:\n",
    "        messagebox.showerror(\"Error\", \"No text could be extracted from the PDF.\")\n",
    "\n",
    "upload_button = tk.Button(root, text=\"Upload Aadhaar PDF\", command=upload_pdf)\n",
    "upload_button.pack(pady=10)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60e8ea8f-f23f-4812-a8cc-6e6698f1a394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images...\n",
      "Loaded 21 images.\n",
      "Training SVM with hyperparameter tuning...\n",
      "Best Parameters: {'C': 0.1, 'kernel': 'linear'}\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.50      0.40         2\n",
      "           1       0.75      0.60      0.67         5\n",
      "\n",
      "    accuracy                           0.57         7\n",
      "   macro avg       0.54      0.55      0.53         7\n",
      "weighted avg       0.63      0.57      0.59         7\n",
      "\n",
      "‚úÖ Prediction for uploaded image: 0 (label simulated)\n",
      "Valid images: 0\n",
      "Invalid images: 21\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 18 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n18 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\DC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\DC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\DC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 207, in fit\n    y = self._validate_targets(y)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\DC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 751, in _validate_targets\n    raise ValueError(\nValueError: The number of classes has to be greater than one; got 1 class\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 127\u001b[0m\n\u001b[0;32m    125\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[0;32m    126\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(SVC(), params, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m--> 127\u001b[0m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[0;32m    130\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(grid\u001b[38;5;241m.\u001b[39mbest_estimator_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvm_model.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1001\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    995\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    996\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    997\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    999\u001b[0m     )\n\u001b[1;32m-> 1001\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:517\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    511\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    515\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    516\u001b[0m     )\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    521\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    522\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 18 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n18 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\DC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\DC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\DC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 207, in fit\n    y = self._validate_targets(y)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\DC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 751, in _validate_targets\n    raise ValueError(\nValueError: The number of classes has to be greater than one; got 1 class\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tkinter import filedialog, Tk\n",
    "\n",
    "# Path to your local image dataset\n",
    "image_folder = r\"C:/Users/DC/Downloads/archive (7)\"\n",
    "\n",
    "# Load images\n",
    "image_data = []\n",
    "labels = []  # We'll simulate labels: 0 or 1\n",
    "\n",
    "print(\"Loading images...\")\n",
    "for idx, filename in enumerate(os.listdir(image_folder)):\n",
    "    if filename.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(image_folder, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (128, 128))  # Resize to common size\n",
    "            img = img.flatten()  # Flatten for SVM input\n",
    "            image_data.append(img)\n",
    "\n",
    "            # Simulated labels: alternate 0 and 1 for now\n",
    "            labels.append(idx % 2)\n",
    "\n",
    "image_data = np.array(image_data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Check if images loaded\n",
    "print(f\"Loaded {len(image_data)} images.\")\n",
    "if len(image_data) == 0:\n",
    "    raise ValueError(\"No images found in the folder. Check the path!\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    image_data, labels, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train + Hyperparameter tuning\n",
    "print(\"Training SVM with hyperparameter tuning...\")\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "svc = svm.SVC()\n",
    "grid = GridSearchCV(svc, param_grid, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Parameters: {grid.best_params_}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, grid.predict(X_test)))\n",
    "\n",
    "\n",
    "# =====================\n",
    "# Predict New Image (GUI File Picker)\n",
    "# =====================\n",
    "def predict_new_image():\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the GUI\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        filetypes=[(\"Image files\", \"*.jpg;*.jpeg;*.png\")]\n",
    "    )\n",
    "    if file_path:\n",
    "        test_img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "        test_img = cv2.resize(test_img, (128, 128)).flatten().reshape(1, -1)\n",
    "        prediction = grid.predict(test_img)[0]\n",
    "        print(f\"‚úÖ Prediction for uploaded image: {prediction} (label simulated)\")\n",
    "    else:\n",
    "        print(\"No image selected.\")\n",
    "\n",
    "\n",
    "# Run prediction\n",
    "predict_new_image()\n",
    "\n",
    "valid = 0\n",
    "invalid = 0\n",
    "\n",
    "for filename in os.listdir(dataset_dir):\n",
    "    if \"valid\" in filename.lower():\n",
    "        valid += 1\n",
    "    else:\n",
    "        invalid += 1\n",
    "\n",
    "print(f\"Valid images: {valid}\")\n",
    "print(f\"Invalid images: {invalid}\")\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "\n",
    "# Path to dataset\n",
    "dataset_dir = os.path.expanduser(\"~/Downloads/archive (7)\")\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Assuming naming convention like: valid_1.jpg / invalid_2.jpg\n",
    "for filename in os.listdir(dataset_dir):\n",
    "    file_path = os.path.join(dataset_dir, filename)\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        continue\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "    img = img / 255.0\n",
    "    X.append(img.flatten())\n",
    "    \n",
    "    if \"valid\" in filename.lower():\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train & hyper-tune SVM\n",
    "params = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "grid = GridSearchCV(SVC(), params, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(grid.best_estimator_, \"svm_model.joblib\")\n",
    "print(\"‚úÖ SVM model trained and saved as 'svm_model.joblib'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c14e6fa1-8943-4ea6-b98a-68ba64ace20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Model trained on 29 samples\n",
      "üß™ Model tested on 13 samples\n",
      "üèÜ Best Parameters: {'C': 1, 'kernel': 'rbf'}\n",
      "\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.57      0.53         7\n",
      "           1       0.40      0.33      0.36         6\n",
      "\n",
      "    accuracy                           0.46        13\n",
      "   macro avg       0.45      0.45      0.45        13\n",
      "weighted avg       0.45      0.46      0.46        13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from joblib import dump\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ‚úÖ Path to image folder\n",
    "dataset_path = os.path.expanduser(\"~/Downloads/archive (7)\")\n",
    "\n",
    "def load_images_with_simulated_invalid(folder):\n",
    "    valid_images, invalid_images, labels = [], [], []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            path = os.path.join(folder, filename)\n",
    "            img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img = cv2.resize(img, (128, 128))\n",
    "            img_flat = img.flatten() / 255.0\n",
    "            valid_images.append(img_flat)\n",
    "            labels.append(1)  # valid\n",
    "\n",
    "            # Simulate invalid image by flipping\n",
    "            invalid_img = cv2.flip(img, 1)\n",
    "            invalid_flat = invalid_img.flatten() / 255.0\n",
    "            invalid_images.append(invalid_flat)\n",
    "            labels.append(0)  # simulated invalid\n",
    "\n",
    "    all_images = np.array(valid_images + invalid_images)\n",
    "    all_labels = np.array(labels)\n",
    "    return all_images, all_labels\n",
    "\n",
    "# ‚úÖ Load with simulated invalids\n",
    "X, y = load_images_with_simulated_invalid(dataset_path)\n",
    "\n",
    "# ‚úÖ Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# ‚úÖ Train with GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "svc = svm.SVC()\n",
    "grid = GridSearchCV(svc, param_grid, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# ‚úÖ Save model\n",
    "dump(grid, \"svm_model.joblib\")\n",
    "\n",
    "# ‚úÖ Predict and report\n",
    "y_pred = grid.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# ‚úÖ Output training summary\n",
    "print(f\"üß† Model trained on {len(X_train)} samples\")\n",
    "print(f\"üß™ Model tested on {len(X_test)} samples\")\n",
    "print(f\"üèÜ Best Parameters: {grid.best_params_}\")\n",
    "print(\"\\nüìä Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dce3a4-b650-42f2-954b-fe5d360b2bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from joblib import load\n",
    "\n",
    "# Load trained model\n",
    "model = load(\"svm_model.joblib\")\n",
    "\n",
    "# Image preprocessing function\n",
    "def preprocess_image(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "    img_flat = img.flatten() / 255.0\n",
    "    return img_flat.reshape(1, -1)\n",
    "\n",
    "# Prediction function\n",
    "def predict_image():\n",
    "    filepath = filedialog.askopenfilename(\n",
    "        filetypes=[(\"Image Files\", \"*.jpg *.jpeg *.png\")]\n",
    "    )\n",
    "    if not filepath:\n",
    "        return\n",
    "\n",
    "    img_array = preprocess_image(filepath)\n",
    "    prediction = model.predict(img_array)[0]\n",
    "\n",
    "    # Show image and prediction\n",
    "    img = Image.open(filepath)\n",
    "    img = img.resize((250, 250))\n",
    "    img_tk = ImageTk.PhotoImage(img)\n",
    "    image_label.configure(image=img_tk)\n",
    "    image_label.image = img_tk\n",
    "\n",
    "    # Display result\n",
    "    if prediction == 1:\n",
    "        result_label.config(text=\"‚úÖ This is a VALID Aadhaar card\", fg=\"green\")\n",
    "    else:\n",
    "        result_label.config(text=\"‚ùå This is an INVALID Aadhaar card\", fg=\"red\")\n",
    "\n",
    "# Tkinter GUI setup\n",
    "root = tk.Tk()\n",
    "root.title(\"Aadhaar Card Validator\")\n",
    "root.geometry(\"400x400\")\n",
    "root.configure(bg=\"white\")\n",
    "\n",
    "title_label = tk.Label(root, text=\"Aadhaar Card Validator\", font=(\"Arial\", 16, \"bold\"), bg=\"white\")\n",
    "title_label.pack(pady=10)\n",
    "\n",
    "image_label = tk.Label(root, bg=\"white\")\n",
    "image_label.pack()\n",
    "\n",
    "upload_btn = tk.Button(root, text=\"Upload Aadhaar Image\", command=predict_image, font=(\"Arial\", 12))\n",
    "upload_btn.pack(pady=10)\n",
    "\n",
    "result_label = tk.Label(root, text=\"\", font=(\"Arial\", 14), bg=\"white\")\n",
    "result_label.pack(pady=10)\n",
    "\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
